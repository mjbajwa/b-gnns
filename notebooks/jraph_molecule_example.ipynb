{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb006c40-c9f8-4927-ba9d-4163b9bd43d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Molecule Example from Jraph colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f040f2db-b5f6-4a26-80ca-23c0da004ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jraph \n",
    "import jax\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "\n",
    "from flax import linen as nn\n",
    "from typing import Sequence\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84300b46-69a3-4485-babf-83d7745a5eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download jraph version of MUTAG.\n",
    "# !wget -P tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/mutag.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11952011-0a7e-4241-b244-e3a467e82be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
    "    nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
    "    nx_graph = nx.DiGraph()\n",
    "    if nodes is None:\n",
    "        for n in range(jraph_graph.n_node[0]):\n",
    "          nx_graph.add_node(n)\n",
    "    else:\n",
    "        for n in range(jraph_graph.n_node[0]):\n",
    "          nx_graph.add_node(n, node_feature=nodes[n])\n",
    "    if edges is None:\n",
    "        for e in range(jraph_graph.n_edge[0]):\n",
    "          nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
    "    else:\n",
    "        for e in range(jraph_graph.n_edge[0]):\n",
    "          nx_graph.add_edge(\n",
    "              int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
    "    return nx_graph\n",
    "\n",
    "\n",
    "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
    "    nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(\n",
    "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5575f466-48ce-4f55-ac05-8e262ea5c77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('tmp/mutag.pickle', 'rb') as f:\n",
    "    mutag_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ddf502a-b524-4f8c-b5ae-1c302704d749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, obj in enumerate(mutag_ds[1:150]):\n",
    "\n",
    "#     g = obj['input_graph']\n",
    "#     print(f'Number of nodes: {g.n_node[0]}')\n",
    "#     print(f'Number of edges: {g.n_edge[0]}')\n",
    "#     print(f'Node features shape: {g.nodes.shape}')\n",
    "#     print(f'Edge features shape: {g.edges.shape}')\n",
    "#     print(f\"Target: {obj['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8c9e4c-c4c3-40ee-84d1-062c587b65e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw_jraph_graph_structure(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8203b42-9c3f-4cd1-b121-6dbb87ff5313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mutag_ds = mutag_ds[:150]\n",
    "test_mutag_ds = mutag_ds[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d1dd9e-40f2-40ff-8a35-cc4a42161f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Padding\n",
    "\n",
    "def _nearest_bigger_power_of_two(x: int) -> int:\n",
    "    \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
    "    y = 2\n",
    "    while y < x:\n",
    "        y *= 2\n",
    "    return y\n",
    "\n",
    "def pad_graph_to_nearest_power_of_two(\n",
    "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
    "    For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
    "    would pad the `GraphsTuple` nodes and edges:\n",
    "    7 nodes --> 8 nodes (2^3)\n",
    "    5 edges --> 8 edges (2^3)\n",
    "    And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
    "    graph and node is added:\n",
    "    8 nodes --> 9 nodes\n",
    "    3 graphs --> 4 graphs\n",
    "    Args:\n",
    "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
    "    Returns:\n",
    "    A graphs_tuple batched to the nearest power of two.\n",
    "    \"\"\"\n",
    "    # Add 1 since we need at least one padding node for pad_with_graphs.\n",
    "    pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
    "    pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
    "    # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
    "    # We do not pad to nearest power of two because the batch size is fixed.\n",
    "    pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
    "    return jraph.pad_with_graphs(\n",
    "        graphs_tuple, \n",
    "        pad_nodes_to, \n",
    "        pad_edges_to,\n",
    "        pad_graphs_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc05a5b-e4db-41c4-a392-a68eb4eb98ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExplicitMLP(nn.Module):\n",
    "    \"\"\"A flax MLP.\"\"\"\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, lyr in enumerate([nn.Dense(feat) for feat in self.features]):\n",
    "          x = lyr(x)\n",
    "          if i != len(self.features) - 1:\n",
    "            x = nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "def make_embed_fn(latent_size):\n",
    "    def embed(inputs):\n",
    "        return nn.Dense(latent_size)(inputs)\n",
    "    return embed\n",
    "\n",
    "def make_mlp(features):\n",
    "    @jraph.concatenated_args\n",
    "    def update_fn(inputs):\n",
    "        return ExplicitMLP(features)(inputs)\n",
    "    return update_fn\n",
    "\n",
    "class GraphNetwork(nn.Module):\n",
    "    \"\"\"A flax GraphNetwork.\"\"\"\n",
    "    mlp_features: Sequence[int]\n",
    "    latent_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, graph):\n",
    "    \n",
    "        # Add a global parameter for graph classification computation\n",
    "        \n",
    "        graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
    "\n",
    "        embedder = jraph.GraphMapFeatures(\n",
    "            embed_node_fn=make_embed_fn(self.latent_size),\n",
    "            embed_edge_fn=make_embed_fn(self.latent_size),\n",
    "            embed_global_fn=make_embed_fn(self.latent_size))\n",
    "        \n",
    "        net = jraph.GraphNetwork(\n",
    "            update_node_fn=make_mlp(self.mlp_features),\n",
    "            update_edge_fn=make_mlp(self.mlp_features),\n",
    "            # The global update outputs size 2 for binary classification.\n",
    "            update_global_fn=make_mlp(self.mlp_features + (1,)))  # pytype: disable=unsupported-operands\n",
    "\n",
    "        return net(embedder(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6fc0396-0a52-4d26-94ec-e72e23e61ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(params, graph, label, net):\n",
    "    \"\"\"Computes loss.\"\"\"\n",
    "    pred_graph = net.apply(params, graph)\n",
    "    preds = jax.nn.log_softmax(pred_graph.globals)\n",
    "    targets = jax.nn.one_hot(label, 2)\n",
    "\n",
    "    # Since we have an extra 'dummy' graph in our batch due to padding, we want\n",
    "    # to mask out any loss associated with the dummy graph.\n",
    "    # Since we padded with `pad_with_graphs` we can recover the mask by using\n",
    "    # get_graph_padding_mask.\n",
    "    mask = jraph.get_graph_padding_mask(pred_graph)\n",
    "\n",
    "    # Cross entropy loss.\n",
    "    # loss = -jnp.mean(preds * targets * mask[:, None])\n",
    "    loss = -jnp.mean(preds * targets)\n",
    "\n",
    "    # Accuracy taking into account the mask.\n",
    "    accuracy = jnp.sum(\n",
    "      (jnp.argmax(pred_graph.globals, axis=1) == label) * mask)/jnp.sum(mask)\n",
    "        \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "066de692-6d52-4d1a-874c-51f5e7e2bd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataset, num_train_steps):\n",
    "\n",
    "    net = GraphNetwork(mlp_features=(128, 128), latent_size=128)\n",
    "\n",
    "    # Get a candidate graph and label to initialize the network.\n",
    "    graph = dataset[0]['input_graph']\n",
    "\n",
    "    # Initialize the network.\n",
    "    params = net.init(jax.random.PRNGKey(42), graph)\n",
    "\n",
    "    opt_init, opt_update = optax.adam(1e-4)\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    compute_loss_fn = functools.partial(compute_loss, net=net)\n",
    "    # We jit the computation of our loss, since this is the main computation.\n",
    "    # Using jax.jit means that we will use a single accelerator. If you want\n",
    "    # to use more than 1 accelerator, use jax.pmap. More information can be\n",
    "    # found in the jax documentation.\n",
    "    compute_loss_fn = jax.jit(jax.value_and_grad(compute_loss_fn, has_aux=True))\n",
    "\n",
    "    for idx in range(num_train_steps):\n",
    "        \n",
    "        graph = dataset[idx % len(dataset)]['input_graph']\n",
    "        label = dataset[idx % len(dataset)]['target']\n",
    "        \n",
    "        # Jax will re-jit your graphnet every time a new graph shape is encountered.\n",
    "        # In the limit, this means a new compilation every training step, which\n",
    "        # will result in *extremely* slow training. To prevent this, pad each\n",
    "        # batch of graphs to the nearest power of two. Since jax maintains a cache\n",
    "        # of compiled programs, the compilation cost is amortized.\n",
    "        graph = pad_graph_to_nearest_power_of_two(graph)\n",
    "\n",
    "        # Remove the label from the input graph/\n",
    "        label = jnp.concatenate([label, jnp.array([0])])\n",
    "        \n",
    "        (loss, acc), grad = compute_loss_fn(params, graph, label)\n",
    "        updates, opt_state = opt_update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(f'step: {idx}, loss: {loss}, acc: {acc}')\n",
    "    \n",
    "    print('Training finished')\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "031d18b4-148c-4078-a4a9-f8beb6ee3c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params = train(train_mutag_ds, num_train_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4da45d6-b90e-4c9c-a596-7189f012b47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset: List[Dict[str, Any]],\n",
    "             params) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \n",
    "    \"\"\"Evaluation Script.\"\"\"\n",
    "    # Transform impure `net_fn` to pure functions with hk.transform.\n",
    "    net = GraphNetwork(mlp_features=[128, 128], latent_size=128)\n",
    "    # Get a candidate graph and label to initialize the network.\n",
    "    graph = dataset[0]['input_graph']\n",
    "    accumulated_loss = 0\n",
    "    accumulated_accuracy = 0\n",
    "    compute_loss_fn = jax.jit(functools.partial(compute_loss, net=net))\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        graph = dataset[idx]['input_graph']\n",
    "        label = dataset[idx]['target']\n",
    "        graph = pad_graph_to_nearest_power_of_two(graph)\n",
    "        label = jnp.concatenate([label, jnp.array([0])])\n",
    "        loss, acc = compute_loss_fn(params, graph, label)\n",
    "        accumulated_accuracy += acc\n",
    "        accumulated_loss += loss\n",
    "        if idx % 50 == 0:\n",
    "          print(f'Evaluated {idx + 1} graphs')\n",
    "    \n",
    "    print('Completed evaluation.')\n",
    "    loss = accumulated_loss / idx\n",
    "    accuracy = accumulated_accuracy / idx\n",
    "    print(f'Eval loss: {loss}, accuracy {accuracy}')\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a928fd77-1858-47fa-ab39-32307a9c5d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(test_mutag_ds, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3a7fba9-ab5f-4349-aec3-b00f16c2fb46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = train_mutag_ds\n",
    "\n",
    "# ### AUC etc. \n",
    "\n",
    "# net = GraphNetwork(mlp_features=[128, 128], latent_size=128)\n",
    "# # Get a candidate graph and label to initialize the network.\n",
    "# graph = dataset[0]['input_graph']\n",
    "# accumulated_loss = 0\n",
    "# accumulated_accuracy = 0\n",
    "# compute_loss_fn = jax.jit(functools.partial(compute_loss, net=net))\n",
    "# preds = []\n",
    "\n",
    "# for idx in range(len(dataset)):\n",
    "#     graph = dataset[idx]['input_graph']\n",
    "#     # label = dataset[idx]['target']\n",
    "#     graph = pad_graph_to_nearest_power_of_two(graph)\n",
    "#     # label = jnp.concatenate([label, jnp.array([0])])\n",
    "#     pred_graph = net.apply(params, graph)\n",
    "#     preds.append(jax.nn.log_softmax(pred_graph.globals).flatten()[0])\n",
    "    \n",
    "# true_y = jnp.array([d['target'] for d in dataset]).flatten()\n",
    "# preds = jnp.exp(jnp.array(preds))\n",
    "# plot_and_print_auc(true_y, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
